{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324e1c84",
   "metadata": {
    "papermill": {
     "duration": 0.00557,
     "end_time": "2023-11-15T15:39:20.568327",
     "exception": false,
     "start_time": "2023-11-15T15:39:20.562757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# This is a demo notebook of running Retriever-Reader Models for science multi-choice exam. \n",
    "\n",
    "## 1. **Retriever-Reader Models**\n",
    "\n",
    "Retriever-Reader models, particularly the Contextualized Retrieval-Augmented Generation (RAG) approach, represent a breakthrough in Large Language Models (LLMs) such as GPT-3. These models integrate a dual-process system:\n",
    "\n",
    "- **The Retriever**: This component is responsible for extracting pertinent documents or passages, such as from Wikipedia, to provide contextual grounding for the LLM's response generation. It's the cornerstone for sourcing external information that enhances the depth and accuracy of the responses.\n",
    "  \n",
    "- **The Reader**: Following retrieval, this component assimilates the gathered information to produce responses that are not only relevant but also informed by the additional context. This functionality is crucial in question-answering scenarios, particularly in educational settings where precision and reliability are key.\n",
    "\n",
    "## 2. **The RAG Components**\n",
    "\n",
    "### Search Engine Integration\n",
    "\n",
    "- **Pyserini with Apache Lucene**: We employ Pyserini, a Python toolkit, for its seamless integration with Apache Lucene, a renowned Java-based search library. Lucene's advanced full-text indexing and search capabilities, combined with Pyserini's Python-friendly interface, form the backbone of our retrieval system.\n",
    "\n",
    "### Ranking Criterion\n",
    "\n",
    "- **BM25 Scoring**: Our system utilizes Lucene's BM25 scoring function for text retrieval. BM25 optimizes document ranking by considering term frequency and document length, providing a balanced approach to information retrieval. This method is crucial for selecting the most relevant passages to inform the LLM's responses.\n",
    "\n",
    "## 3. **LLM Integration**\n",
    "\n",
    "In this demo, we integrate a 70-billion-parameter Large Language Model using the Llama2 structure. This model choice is strategic for its advanced capabilities in understanding and generating human-like responses. To accommodate computational constraints, we employ a flexible sharding technique, ensuring optimal performance within Kaggle's GPU environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f5400",
   "metadata": {
    "papermill": {
     "duration": 0.004805,
     "end_time": "2023-11-15T15:39:20.578592",
     "exception": false,
     "start_time": "2023-11-15T15:39:20.573787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step 1: Setup libraries and Java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e6600d3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-15T15:39:20.590613Z",
     "iopub.status.busy": "2023-11-15T15:39:20.590169Z",
     "iopub.status.idle": "2023-11-15T15:42:02.769033Z",
     "shell.execute_reply": "2023-11-15T15:42:02.767697Z"
    },
    "papermill": {
     "duration": 162.18738,
     "end_time": "2023-11-15T15:42:02.771219",
     "exception": false,
     "start_time": "2023-11-15T15:39:20.583839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'flexible-LLM-sharding'...\r\n",
      "remote: Enumerating objects: 48, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (48/48), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (47/47), done.\u001b[K\r\n",
      "remote: Total 48 (delta 22), reused 0 (delta 0), pack-reused 0\u001b[K\r\n",
      "Receiving objects: 100% (48/48), 261.95 KiB | 2.82 MiB/s, done.\r\n",
      "Resolving deltas: 100% (22/22), done.\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "!pip install -q /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl --ignore-installed\n",
    "# !pip install -q /kaggle/input/lucene-deps/pyserini-0.22.1-py3-none-any.whl --ignore-installed\n",
    "!pip install -q pyserini\n",
    "!git clone https://github.com/Robot-Eyes/flexible-LLM-sharding.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a176953a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T15:42:02.784295Z",
     "iopub.status.busy": "2023-11-15T15:42:02.783988Z",
     "iopub.status.idle": "2023-11-15T15:42:04.679987Z",
     "shell.execute_reply": "2023-11-15T15:42:04.678919Z"
    },
    "papermill": {
     "duration": 1.905349,
     "end_time": "2023-11-15T15:42:04.682312",
     "exception": false,
     "start_time": "2023-11-15T15:42:02.776963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JAVA_HOME=/kaggle/input/lucene-deps/openjdk-20.0.2_linux-x64_bin/jdk-20.0.2/\n",
      "/kaggle/input/lucene-deps/openjdk-20.0.2_linux-x64_bin/jdk-20.0.2/\r\n",
      "env: PATH=/kaggle/input/lucene-deps/openjdk-20.0.2_linux-x64_bin/jdk-20.0.2/bin:/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "/kaggle/input/lucene-deps/openjdk-20.0.2_linux-x64_bin/jdk-20.0.2/bin:/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\r\n"
     ]
    }
   ],
   "source": [
    "%env JAVA_HOME=/kaggle/input/lucene-deps/openjdk-20.0.2_linux-x64_bin/jdk-20.0.2/\n",
    "!echo $JAVA_HOME\n",
    "%env PATH=/kaggle/input/lucene-deps/openjdk-20.0.2_linux-x64_bin/jdk-20.0.2/bin:{os.environ['PATH']}\n",
    "!echo $PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e11b8",
   "metadata": {
    "papermill": {
     "duration": 0.005994,
     "end_time": "2023-11-15T15:42:04.694540",
     "exception": false,
     "start_time": "2023-11-15T15:42:04.688546",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step 2: RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffff74bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T15:42:04.708165Z",
     "iopub.status.busy": "2023-11-15T15:42:04.707861Z",
     "iopub.status.idle": "2023-11-15T15:42:24.633808Z",
     "shell.execute_reply": "2023-11-15T15:42:24.632870Z"
    },
    "papermill": {
     "duration": 19.935511,
     "end_time": "2023-11-15T15:42:24.636215",
     "exception": false,
     "start_time": "2023-11-15T15:42:04.700704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def retrieval_row(\n",
    "    searcher: LuceneSearcher,\n",
    "    row: pd.Series,\n",
    "    top_k: int,\n",
    "    jsonl: bool,\n",
    "    context_template: str,\n",
    "    max_length: int,\n",
    "    merge_method: str = \"none\",\n",
    "    search_method: str = \"normal\",\n",
    ") -> list[str]:\n",
    "    if search_method == \"normal\":\n",
    "        query = \"{prompt} {A} {B} {C} {D} {E}\".format(**row.to_dict())\n",
    "        hits = searcher.search(q=query, k=top_k)\n",
    "\n",
    "        retrieved_results: list[dict[str, Any]] = []\n",
    "        for hit in hits:\n",
    "            if jsonl:\n",
    "                hit_dict = json.loads(hit.raw)\n",
    "                hit_dict[\"contents\"] = context_template.format_map(hit_dict)\n",
    "            else:\n",
    "                hit_dict = {\"contents\": hit.raw}\n",
    "            retrieved_results.append(hit_dict)\n",
    "    elif search_method in [\"query_by_each_choice\", \"query_by_each_choice_kcut\"]:\n",
    "        retrieved_results_list = []\n",
    "        row_dict = row.to_dict()\n",
    "        prompt = row_dict[\"prompt\"]\n",
    "        for key in [\"A\", \"B\", \"C\", \"D\", \"E\"]:\n",
    "            query = \" \".join([prompt, row_dict[key]])\n",
    "            hits = searcher.search(query, k=top_k)\n",
    "            retrieved_results = []\n",
    "            for _k in range(top_k):\n",
    "                _res = json.loads(hits[_k].raw)\n",
    "                _res[\"score\"] = hits[_k].score\n",
    "                retrieved_results.append(_res)\n",
    "            retrieved_results_list.append(retrieved_results)\n",
    "\n",
    "        retrieved_results_df = pd.DataFrame([item for sublist in retrieved_results_list for item in sublist])\n",
    "        scores = retrieved_results_df[\"score\"].values\n",
    "        # Add rank score, which is much bigger than BM25 score.\n",
    "        scores = scores.reshape(5, top_k) + (np.arange(top_k)[::-1] * 10000.0)\n",
    "        scores = scores.reshape(5 * top_k)\n",
    "        retrieved_results_df[\"rank_bm25_score\"] = scores\n",
    "        retrieved_results_df.drop_duplicates(\"id\", keep=\"first\", inplace=True)\n",
    "        retrieved_results_df.sort_values(\"rank_bm25_score\", ascending=False, inplace=True)\n",
    "        retrieved_results = retrieved_results_df.to_dict(\"records\")\n",
    "        # Be careful that `retrieved_results` here can contain up to p.top_k * 5 candidates.\n",
    "        if search_method == \"query_by_each_choice_kcut\":\n",
    "            retrieved_results = retrieved_results[:top_k]\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unexpected value search_method={search_method}\")\n",
    "\n",
    "    if merge_method == \"none\":\n",
    "        pass\n",
    "    elif merge_method == \"mergev2\":\n",
    "        merged_results = []\n",
    "        contents = \"\"\n",
    "        for _k, res in enumerate(retrieved_results):\n",
    "            if len(contents) > 0 and len(contents) + len(res[\"contents\"]) + 1 > max_length:\n",
    "                merged_results.append({\"contents\": contents})\n",
    "                contents = \"\"\n",
    "                # print(f\"[DEBUG] merged until {_k=}\")\n",
    "            if len(contents) > 0:\n",
    "                contents += \"\\n\"\n",
    "            contents += res[\"contents\"]\n",
    "        if len(contents) > 0:\n",
    "            merged_results.append({\"contents\": contents})\n",
    "            # print(f\"[DEBUG] merged until {len(retrieved_results)}\")\n",
    "        retrieved_results = merged_results\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] Unexpected value merge_method={merge_method}\")\n",
    "\n",
    "    _context_list: list[str] = [res[\"contents\"][:max_length] for res in retrieved_results]\n",
    "    return _context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9deed68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T15:42:24.650488Z",
     "iopub.status.busy": "2023-11-15T15:42:24.650155Z",
     "iopub.status.idle": "2023-11-15T15:42:24.657801Z",
     "shell.execute_reply": "2023-11-15T15:42:24.656953Z"
    },
    "papermill": {
     "duration": 0.017198,
     "end_time": "2023-11-15T15:42:24.659973",
     "exception": false,
     "start_time": "2023-11-15T15:42:24.642775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_lucene_retrieval(\n",
    "    df,\n",
    "    index_dir: str,\n",
    "    jsonl: bool,\n",
    "    max_length: int = 2000,\n",
    "    top_k: int = 5,\n",
    "    bm25_k1: float = 0.9,\n",
    "    bm25_b: float = 0.4,\n",
    "    n_jobs: int = 128,\n",
    "    merge_method: str = \"none\",\n",
    "    search_method: str = \"normal\",\n",
    "):\n",
    "    \n",
    "    # initialize LuceneSearcher\n",
    "    searcher = LuceneSearcher(index_dir)\n",
    "    searcher.set_bm25(k1=bm25_k1, b=bm25_b)\n",
    "\n",
    "    context_template = \"{contents}\"\n",
    "\n",
    "    contexts: list[list[str]] = Parallel(n_jobs, backend=\"threading\", verbose=1)(\n",
    "        delayed(retrieval_row)(searcher, row, top_k, jsonl, context_template, max_length, merge_method, search_method)\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df))\n",
    "    )\n",
    "    context_df = pd.DataFrame(contexts)\n",
    "        \n",
    "    return context_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27980de7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T15:42:24.674332Z",
     "iopub.status.busy": "2023-11-15T15:42:24.673848Z",
     "iopub.status.idle": "2023-11-15T15:42:24.720046Z",
     "shell.execute_reply": "2023-11-15T15:42:24.719128Z"
    },
    "papermill": {
     "duration": 0.055129,
     "end_time": "2023-11-15T15:42:24.722040",
     "exception": false,
     "start_time": "2023-11-15T15:42:24.666911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/train.csv\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36446495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T15:42:24.736073Z",
     "iopub.status.busy": "2023-11-15T15:42:24.735775Z",
     "iopub.status.idle": "2023-11-15T15:43:50.647831Z",
     "shell.execute_reply": "2023-11-15T15:43:50.646780Z"
    },
    "papermill": {
     "duration": 85.921365,
     "end_time": "2023-11-15T15:43:50.649957",
     "exception": false,
     "start_time": "2023-11-15T15:42:24.728592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Nov 15, 2023 3:42:24 PM org.apache.lucene.store.MMapDirectory lookupProvider\n",
      "WARNING: You are running with Java 20 or later. To make full use of MMapDirectory, please update Apache Lucene.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f8a4343cf645be8a6b5d7135a5dbfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   31.6s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=4)]: Done 200 out of 200 | elapsed:  1.4min finished\n"
     ]
    }
   ],
   "source": [
    "context_df = run_lucene_retrieval(df, index_dir='/kaggle/input/lucene-wikipedia-en/',\n",
    "                                jsonl=True, max_length=2000, top_k=2, n_jobs=4)\n",
    "\n",
    "for col_idx in range(context_df.shape[1]):\n",
    "    df[f\"context_{col_idx}\"] = context_df.iloc[:, col_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c210e",
   "metadata": {
    "papermill": {
     "duration": 0.006614,
     "end_time": "2023-11-15T15:43:50.663607",
     "exception": false,
     "start_time": "2023-11-15T15:43:50.656993",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Step 3: Run LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98efb629",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T15:43:50.678027Z",
     "iopub.status.busy": "2023-11-15T15:43:50.677734Z",
     "iopub.status.idle": "2023-11-15T15:43:50.682108Z",
     "shell.execute_reply": "2023-11-15T15:43:50.681240Z"
    },
    "papermill": {
     "duration": 0.013633,
     "end_time": "2023-11-15T15:43:50.683987",
     "exception": false,
     "start_time": "2023-11-15T15:43:50.670354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys, gc\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc15685",
   "metadata": {
    "papermill": {
     "duration": 0.006155,
     "end_time": "2023-11-15T15:43:50.696665",
     "exception": false,
     "start_time": "2023-11-15T15:43:50.690510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generate prompt pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9982b395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T15:43:50.711338Z",
     "iopub.status.busy": "2023-11-15T15:43:50.710844Z",
     "iopub.status.idle": "2023-11-15T15:43:50.753032Z",
     "shell.execute_reply": "2023-11-15T15:43:50.752365Z"
    },
    "papermill": {
     "duration": 0.051342,
     "end_time": "2023-11-15T15:43:50.754895",
     "exception": false,
     "start_time": "2023-11-15T15:43:50.703553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_input_prompts_with_context(df):\n",
    "\n",
    "    instruction_str = (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "                 \"Write a response that appropriately completes the request.\\n\\n### \"\n",
    "                 \"Instruction:\\nYou are an expert in STEM fields. Your task is to analyze the question and the proposed statement below. \"\n",
    "                \"If the statement is correct, respond True; if it is not correct, respond False. \"\n",
    "                   \"As a potential aid for you, some background context is provide, and please note that they could be irrelevant to the question.\"\n",
    "                 \"\\n### ### Context:\\ncontext_text\\nQuestion: \")\n",
    "    \n",
    "    prompt_prefix_list = pd.Series([instruction_str.replace('context_text', row.context_0+row.context_1) \n",
    "                                    for _,row in df.iterrows()]) + df['prompt'] + \"\\nProposed answer: \"\n",
    "    prompt_suffix_list = [tuple(f\"{row[letter]}\\n\\n### Response:\\n\" for letter in \"ABCDE\")\n",
    "                          for _,row in df.iterrows()]\n",
    "\n",
    "    return list(zip(prompt_prefix_list, prompt_suffix_list))\n",
    "\n",
    "input_prompts = prepare_input_prompts_with_context(df)\n",
    "    \n",
    "with open('input_prompts.pkl', 'wb') as file:\n",
    "    pickle.dump(input_prompts, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3ac93b",
   "metadata": {
    "papermill": {
     "duration": 0.006361,
     "end_time": "2023-11-15T15:43:50.767701",
     "exception": false,
     "start_time": "2023-11-15T15:43:50.761340",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create symlinks from kaggle datasets to fake cached model. This step could be skipped on local machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46d0ebc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T15:43:50.781815Z",
     "iopub.status.busy": "2023-11-15T15:43:50.781271Z",
     "iopub.status.idle": "2023-11-15T15:43:50.823877Z",
     "shell.execute_reply": "2023-11-15T15:43:50.822975Z"
    },
    "papermill": {
     "duration": 0.05177,
     "end_time": "2023-11-15T15:43:50.825833",
     "exception": false,
     "start_time": "2023-11-15T15:43:50.774063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs('./platypus2_checkpoints', exist_ok=True)\n",
    "for part in [1, 2]:\n",
    "    source_dir = Path(f\"/kaggle/input/platypus2-70b-instruct-part{part}\")\n",
    "    for path in source_dir.glob(\"*\"):\n",
    "        try:\n",
    "            (Path('./platypus2_checkpoints') / path.name).symlink_to(path)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39ed04cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T15:43:50.840098Z",
     "iopub.status.busy": "2023-11-15T15:43:50.839502Z",
     "iopub.status.idle": "2023-11-15T16:08:16.669164Z",
     "shell.execute_reply": "2023-11-15T16:08:16.668112Z"
    },
    "papermill": {
     "duration": 1465.839125,
     "end_time": "2023-11-15T16:08:16.671469",
     "exception": false,
     "start_time": "2023-11-15T15:43:50.832344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\r\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n",
      "Namespace(model_path='./platypus2_checkpoints', prompt_pickle='input_prompts.pkl', output_file='platypus2_output_score.pkl', num_batch=1, layer_num_per_shard=8, storage_location='cpu', max_activation_in_cpu=100, data_parallel=False, disk_folder='./temp', num_gen_token=1)\r\n",
      "cuda:0 | shards:   0%|                                    | 0/6 [00:00<?, ?it/s]\r\n",
      "cuda:0 | shards:  17%|████▌                      | 1/6 [03:36<18:04, 216.88s/it]\r\n",
      "cuda:0 | shards:  33%|█████████                  | 2/6 [07:50<15:52, 238.24s/it]\r\n",
      "cuda:0 | shards:  50%|█████████████▌             | 3/6 [11:51<11:58, 239.58s/it]\r\n",
      "cuda:0 | shards:  67%|██████████████████         | 4/6 [15:50<07:59, 239.60s/it]\r\n",
      "cuda:0 | shards:  83%|██████████████████████▌    | 5/6 [19:57<04:02, 242.26s/it]\r\n",
      "cuda:0 | shards: 100%|███████████████████████████| 6/6 [24:01<00:00, 240.30s/it]\r\n",
      "cuda:0 loaded 42 layers in 865s\r\n",
      "\r\n",
      "cuda:1 | shards: 100%|███████████████████████████| 6/6 [24:02<00:00, 240.43s/it]\r\n",
      "cuda:1 loaded 41 layers in 835s\r\n"
     ]
    }
   ],
   "source": [
    "!python flexible-LLM-sharding/main.py --layer_num_per_shard 8 --storage_location cpu --num_batch 1 \\\n",
    "                        --model_path ./platypus2_checkpoints \\\n",
    "                        --prompt_pickle input_prompts.pkl --output_file platypus2_output_score.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ff7dd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-15T16:08:16.688923Z",
     "iopub.status.busy": "2023-11-15T16:08:16.688499Z",
     "iopub.status.idle": "2023-11-15T16:08:16.776450Z",
     "shell.execute_reply": "2023-11-15T16:08:16.775401Z"
    },
    "papermill": {
     "duration": 0.098982,
     "end_time": "2023-11-15T16:08:16.778479",
     "exception": false,
     "start_time": "2023-11-15T16:08:16.679497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top1 : 131/200, top2 : 33/200, top3 : 22/200 (total=186 / 200)\n",
      "Accuracy: 65.5%, map3: 77.4%\n"
     ]
    }
   ],
   "source": [
    "with open('platypus2_output_score.pkl', 'rb') as file:\n",
    "    output_scores = pickle.load(file)\n",
    "        \n",
    "n = len(output_scores)\n",
    "for i, scores in enumerate(output_scores):\n",
    "    # Token #5852 is true, #7700 is false.\n",
    "    first_token_scores = scores[:, 0, [5852, 7700]]\n",
    "    positive_scores = first_token_scores[:, 0] / first_token_scores.sum(axis=-1)\n",
    "    top3 = np.argsort(positive_scores)[::-1][:3]\n",
    "    df.loc[i, \"prediction\"] = \" \".join([\"ABCDE\"[j] for j in top3])\n",
    "\n",
    "# Display performances if train set is used\n",
    "if \"answer\" in df.columns:\n",
    "    df[\"top_1\"] = df[\"prediction\"].apply(lambda x:x[0])\n",
    "    df[\"top_2\"] = df[\"prediction\"].apply(lambda x:x[2])\n",
    "    df[\"top_3\"] = df[\"prediction\"].apply(lambda x:x[4])\n",
    "\n",
    "    top_i = [(df[f\"top_{i}\"] == df[\"answer\"]).sum() for i in [1, 2, 3]]\n",
    "    print(f\"top1 : {top_i[0]}/{n}, top2 : {top_i[1]}/{n}, top3 : {top_i[2]}/{n} (total={sum(top_i)} / {n})\")\n",
    "    print(f\"Accuracy: {100*top_i[0]/n:.1f}%, map3: {100*(top_i[0] + top_i[1]*1/2 + top_i[2]*1/3).sum()/n:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2299ea96",
   "metadata": {
    "papermill": {
     "duration": 0.007978,
     "end_time": "2023-11-15T16:08:16.840066",
     "exception": false,
     "start_time": "2023-11-15T16:08:16.832088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 6169864,
     "sourceId": 54662,
     "sourceType": "competition"
    },
    {
     "datasetId": 3238926,
     "sourceId": 5632975,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3682312,
     "sourceId": 6388591,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3756597,
     "sourceId": 6499124,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3725834,
     "sourceId": 6581790,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4004096,
     "sourceId": 6969882,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1742.412409,
   "end_time": "2023-11-15T16:08:19.270486",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-15T15:39:16.858077",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "094645ccceb343339318f1cb87ce94cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2e122f1de0ff42228cc4ac3ba62c1dba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3149316e29dd4d90b9befd6273e29ca4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "42ca72a136344253bf2a9a9a5226f021": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b9b24f7955f84b8b8dfe125ce7354991",
       "max": 200.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_094645ccceb343339318f1cb87ce94cd",
       "value": 200.0
      }
     },
     "453e87fd61c8471cbcf398d5dc15e6ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4f77d11ca2ba4ef2a42eaf651d4c0bd2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2e122f1de0ff42228cc4ac3ba62c1dba",
       "placeholder": "​",
       "style": "IPY_MODEL_df5a05839c944abca58c3331bb2e38e1",
       "value": " 200/200 [01:20&lt;00:00,  3.56it/s]"
      }
     },
     "727a28bef57041aaba0d9a0ad9d72ad8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b9b24f7955f84b8b8dfe125ce7354991": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "df5a05839c944abca58c3331bb2e38e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e9cad138f6e3440e9fbbf2b871e80588": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_727a28bef57041aaba0d9a0ad9d72ad8",
       "placeholder": "​",
       "style": "IPY_MODEL_453e87fd61c8471cbcf398d5dc15e6ab",
       "value": "100%"
      }
     },
     "f5f8a4343cf645be8a6b5d7135a5dbfa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e9cad138f6e3440e9fbbf2b871e80588",
        "IPY_MODEL_42ca72a136344253bf2a9a9a5226f021",
        "IPY_MODEL_4f77d11ca2ba4ef2a42eaf651d4c0bd2"
       ],
       "layout": "IPY_MODEL_3149316e29dd4d90b9befd6273e29ca4"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
